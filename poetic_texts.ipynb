{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "metadata": {
        "id": "uB43uQKN_27u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "text = open(filepath, 'rb').read().decode(encoding='utf-8').lower()\n",
        "text = text[300000:800000]\n",
        "\n",
        "characters = sorted(set(text))\n",
        "\n",
        "char_to_index = dict((c, i) for i, c in enumerate(characters))\n",
        "index_to_char = dict((i, c) for i, c in enumerate(characters))\n",
        "\n",
        "SEQ_LENGTH = 40\n",
        "STEP_SIZE = 3\n",
        "\n",
        "sentences = []\n",
        "next_characters = []\n",
        "\n",
        "for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE):\n",
        "    sentences.append(text[i: i + SEQ_LENGTH])\n",
        "    next_characters.append(text[i + SEQ_LENGTH])\n",
        "\n",
        "x = np.zeros((len(sentences), SEQ_LENGTH, len(characters)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(characters)), dtype=np.bool)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, character in enumerate(sentence):\n",
        "        x[i, t, char_to_index[character]] = 1\n",
        "    y[i, char_to_index[next_characters[i]]] = 1\n",
        "\n",
        "'''\n",
        "# Initialize the Sequential model\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(SEQ_LENGTH, len(characters))))\n",
        "model.add(Dense(len(characters)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))\n",
        "\n",
        "model.fit(x, y, batch_size=256, epochs=4)\n",
        "\n",
        "# Save the model\n",
        "model.save('textgenerator.model')\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVA0zH0l_3-y",
        "outputId": "ec66625a-ec71-420f-eb90-04e90c655efc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-dd5987b4544e>:21: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  x = np.zeros((len(sentences), SEQ_LENGTH, len(characters)), dtype=np.bool)\n",
            "<ipython-input-8-dd5987b4544e>:22: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y = np.zeros((len(sentences), len(characters)), dtype=np.bool)\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "651/651 [==============================] - 117s 177ms/step - loss: 2.7092\n",
            "Epoch 2/4\n",
            "651/651 [==============================] - 112s 171ms/step - loss: 2.3005\n",
            "Epoch 3/4\n",
            "651/651 [==============================] - 112s 172ms/step - loss: 2.1810\n",
            "Epoch 4/4\n",
            "651/651 [==============================] - 111s 171ms/step - loss: 2.0958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model= tf.keras.models.load_model('textgenerator.model')"
      ],
      "metadata": {
        "id": "Ojvapz41D61E"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature= 1.0):\n",
        "  preds= np.asarray(preds).astype('float64')\n",
        "  preds= np.log(preds)/temperature\n",
        "  exp_preds= np.exp(preds)\n",
        "  preds= exp_preds/ np.sum(exp_preds)\n",
        "  probas= np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)"
      ],
      "metadata": {
        "id": "zAXTwmE8ELLK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(length, temperature):\n",
        "  start_index= random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "  generated=''\n",
        "  sentence= text[start_index : start_index + SEQ_LENGTH]\n",
        "  generated+= sentence\n",
        "  for i in range(length):\n",
        "    x= np.zeros((1, SEQ_LENGTH, len(characters)))\n",
        "    for t, character in enumerate(sentence):\n",
        "      x[0, t, char_to_index[character]]= 1\n",
        "\n",
        "    prediction= model.predict(x, verbose=0)[0]\n",
        "    next_index= sample(prediction, temperature)\n",
        "    next_character= index_to_char[next_index]\n",
        "\n",
        "    generated+= next_character\n",
        "    sentence = sentence[1:] + next_character\n",
        "  return generated\n"
      ],
      "metadata": {
        "id": "MCJ5Yg8RKXAk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('---------0.2---------')\n",
        "print(generate_text(300, 0.2))\n",
        "print('---------0.4---------')\n",
        "print(generate_text(300, 0.4))\n",
        "print('---------0.6---------')\n",
        "print(generate_text(300, 0.6))\n",
        "print('---------0.8---------')\n",
        "print(generate_text(300, 0.8))\n",
        "print('---------1---------')\n",
        "print(generate_text(300, 1.0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EidKop-MRf7",
        "outputId": "f4518caf-30ab-4c66-f609-8fb56657bb8d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------0.2---------\n",
            "kill'd, he's dead!\n",
            "\n",
            "juliet:\n",
            "can heaven beat the head the sore the have the warte the seather and the with the parte the sare to the bute the are the and and and and and mand the beather in the sond and and and the mear and and the pare the sore to the pare the seall of the his the the meres and and and the will the proust the the the with\n",
            "---------0.4---------\n",
            "er\n",
            "and the lord hastings, who attended have the hard and hear hat ay his for thae and and at in the for the cour the fore hourd.\n",
            "\n",
            "pardowe:\n",
            "whith the mare the hare the each the allous seast\n",
            "and betere in this beather counted i hand and this the ming casten steed and of mere the deart of and nower of mear the preast the pare the have the wo\n",
            "---------0.6---------\n",
            "our heart for sending me about,\n",
            "to catch sere hest yous in thy beantstale and the porting and then?\n",
            "browi:\n",
            "shee, the wanco dusest the undentore's then,\n",
            "to the way hames, and benter and the comas it the ard of out here cous ford fearind thou sof in ard the corpere,\n",
            "lit. ay your swall your in her have thee wheting comencd.\n",
            "in shears seall t\n",
            "---------0.8---------\n",
            "--\n",
            "and breathed such life with kisses in adp at yurd ors;\n",
            "add at bered-'ssertar! all figr thousw youle the heprparter and me hing\n",
            "youte, be\n",
            "shongs maves als dovere shearing,\n",
            "woull, ances male thay under, seam weal the bulse wiind the corfare poperise:\n",
            "wond breis aculd ding.\n",
            "\n",
            "hirbeydel har ires the to sould mest of whin mery ie fricher?\n",
            "\n",
            "b\n",
            "---------1---------\n",
            "h me, and we will make short work;\n",
            "for, the undours, while us besrast' firk,\n",
            "too, to gooft's in youngsten, entie and?\n",
            "tour simuy tre iastore fothanes jyurd,\n",
            "'ndsise manber thas thambo,\n",
            "and ais netr sthreneros\n",
            "this ndect to mead aspief rute s anstre, dsiford ave sse.\n",
            "\n",
            "wacher'so:\n",
            "not eheow us in tomus dpeand day to ani?\n",
            "thyee of' weech me, \n"
          ]
        }
      ]
    }
  ]
}